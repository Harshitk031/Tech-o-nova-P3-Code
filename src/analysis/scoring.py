# src/analysis/scoring.py

def calculate_scores(recommendation, plan_data, stats_data, hypopg_delta=None):
    """
    Calculates confidence and impact scores for a given recommendation.

    Args:
        recommendation (dict): The recommendation generated by the rules engine.
        plan_data (dict): The parsed plan data.
        stats_data (dict): Data from pg_stat_statements (e.g., total_exec_time).
        hypopg_delta (dict, optional): The result of a HypoPG simulation.

    Returns:
        A tuple of (confidence_score, impact_tier).
    """
    
    # --- Confidence Score Calculation (0.0 to 1.0) ---
    confidence = 0.0
    
    # Rule specificity is the base score
    if recommendation['type'] == 'MISSING_INDEX':
        confidence = 0.7  # High base confidence for this common, clear-cut issue
    
    # Strong plan evidence (e.g., high filter selectivity) increases confidence
    if plan_data.get('Rows Removed by Filter', 0) > plan_data.get('Actual Rows', 0) * 10:
        confidence += 0.1
        
    # A positive HypoPG simulation provides the strongest confirmation
    if hypopg_delta and "Index" in hypopg_delta.get('after_node_type', ''):
        confidence += 0.2
        
    confidence = min(confidence, 1.0) # Cap the score at 1.0

    # --- Impact Tier Calculation (Low/Medium/High) ---
    impact = 'Low'
    
    # Severity of the operator change is a strong indicator of impact
    if "Seq Scan" in str(plan_data):
        impact = 'Medium'
        
    # High execution time from stats indicates high impact
    # Assuming stats_data contains 'total_exec_time'
    if stats_data.get('total_exec_time', 0) > 1000: # Example threshold: >1 second total
        impact = 'Medium'

    # If both conditions are met, or HypoPG shows a massive cost reduction, impact is high
    if (impact == 'Medium' and stats_data.get('total_exec_time', 0) > 1000) or \
       (hypopg_delta and hypopg_delta.get('cost_reduction_percent', 0) > 90):
        impact = 'High'
        
    return round(confidence, 2), impact


if __name__ == '__main__':
    # --- This section simulates running the scoring on a recommendation ---

    # 1. A sample recommendation from your rules engine
    sample_recommendation = {
        "type": "MISSING_INDEX",
        "suggested_action": "CREATE INDEX ON orders (customer_id);"
    }

    # 2. Evidence collected from other parts of the pipeline
    sample_plan = {'Node Type': 'Seq Scan', 'Rows Removed by Filter': 99901, 'Actual Rows': 99}
    sample_stats = {'total_exec_time': 5200.5} # Query took ~5.2s total
    sample_hypopg = {'after_node_type': 'Index Scan', 'cost_reduction_percent': 99.8}

    # 3. Calculate the scores
    confidence_score, impact_tier = calculate_scores(
        sample_recommendation,
        sample_plan,
        sample_stats,
        sample_hypopg
    )

    # 4. Print the final, enriched recommendation
    print("--- üèÜ Enriched Recommendation ---")
    sample_recommendation['confidence'] = confidence_score
    sample_recommendation['impact'] = impact_tier
    
    for key, value in sample_recommendation.items():
        print(f"- {key.title()}: {value}")
